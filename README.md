### Hi there ðŸ‘‹
<!--
- ðŸ”­ My research focuses on **AI Safety and Reliability**. Toward this goal, I am working on the intersection of self-supervision, generative modeling, spatial/temporal modeling for image/video understanding, label-efficient learning, adversarial attacks/defenses, out-of-distribution generalization, and privacy-preserving.
-->
<!--
- ðŸ”­ My research interests are robust perception to understand and explain AI behavior, temporal perception, self-learning, AI safety, and reliability.
-->
- ðŸ”­ My research interests are robust visual perception by understanding and explaining AI behavior through adversarial machine learning, temporal perception, representation learning (self-supervision, self-distillation, self-critique), and configuring the role of language models (LLMs) in building visual AI systems.
- ðŸŒ± You are welcome to explore my research work along with the provided code below. Seven of the papers are accepted as **Oral/Spotlight** at ICLR, NeurIPS, AAAI, CVPR, BMVC, and ACCV.
- ðŸ“« How to reach me: muz.pak@gmail.com
- âš¡ Fun fact: I am really into fitness and thinking of joining the GYM for quite some time now :smile:

<!-- - ðŸ‘¯ Iâ€™m looking to collaborate on 
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ˜„ Pronouns: he/him/his

![GitHub Stats](https://github-readme-stats.vercel.app/api?username=muzammal-naseer&theme=radical)
-->
:seedling: **Repositories**

|Topic|Application|Paper|Repo|Venue
|---|:---:|:---:|:---:|:---:|
Self-supervision | Multi-Spectral Satellite Imagery | [Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery](https://arxiv.org/abs/2403.05419)|[satmae_pp](https://github.com/techmn/satmae_pp)|CVPR'24|
Vision-Language Learning | Video grounding | [Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding](https://arxiv.org/abs/2401.00901)|[Video-GroundingDINO](https://github.com/TalalWasim/Video-GroundingDINO)|CVPR'24|
Vision-Language Learning| Language Driven  VLM for Remote Sensing | [Geochat: Grounded large vision-language model for remote sensing](https://arxiv.org/abs/2311.15826) | [GeoChat](https://github.com/mbzuai-oryx/GeoChat)|CVPR'24|
Vision-Language Learning| Leaverging LLM to generate complex scenes (Zero-Shot) | [LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts](https://arxiv.org/abs/2310.10640) | [llmblueprint](https://github.com/hananshafi/llmblueprint)| ICLR'24|
Self-supervision | Self-structural Alignment of Foundational Models (Zero-Shot) | [Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment](https://arxiv.org/abs/2308.12960) | [S3A](https://github.com/sheng-eatamath/S3A) | AAAI'24-Oral| 
Vision-Language Learning | Test-Time Alignment of Foundational Models (Zero-Shot) | [Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization](https://arxiv.org/abs/2311.01459) | [PromptAlign](https://github.com/jameelhassan/PromptAlign) | NeurIPS'23|
Vision-Language Learning | Regulating Foundational Models| [Self-regulating Prompts: Foundational Model Adaptation without Forgetting](https://arxiv.org/abs/2307.06948)|[PromptSRC](https://github.com/muzairkhattak/PromptSRC)|ICCV'23|
Network Engineering| Video Recognition| [Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition](https://arxiv.org/abs/2307.06947)| [Video-FocalNets](https://github.com/TalalWasim/Video-FocalNets)| ICCV'23
Vision-Language Learning| Face Anti-spoofing | [FLIP: Cross-domain Face Anti-spoofing with Language Guidance](https://muzammal-naseer.com/TBA)|[FLIP](https://github.com/koushiksrivats/FLIP)| ICCV'23
3D Medical Segmentation| Adversarial Training | [Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation](https://arxiv.org/abs/2307.07269)|[VAFA](https://github.com/asif-hanif/vafa)|MICCAI'23
Vision-Language Learning |Facial Privacy| [CLIP2Protect: Protecting Facial Privacy Using Text-Guided Makeup via Adversarial Latent Search](https://arxiv.org/abs/2306.10008)|[Clip2Protect](https://github.com/fahadshamshad/Clip2Protect)|CVPR'23
Vision-Language Learning | Video Recognition (Zero-shot)| [Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting](https://arxiv.org/abs/2304.03307)|[Vita-CLIP](https://github.com/TalalWasim/Vita-CLIP)|CVPR'23|
Prompt learning | Image Recognition (Category Discovery)| [PromptCAL for Generalized Novel Category Discovery](https://arxiv.org/abs/2212.05590)|[PromptCAL](https://github.com/sheng-eatamath/PromptCAL)|CVPR'23|
Prompt learning | Adversarial Attack | [Boosting Adversarial Transferability using Dynamic Cues](https://openreview.net/forum?id=SZynfVLGd5) | [DCViT-AT](https://github.com/Muzammal-Naseer/DCViT-AT)|ICLR'23|
Self-supervision| Video Recognition | [Self-Supervised Video Transformer](https://openaccess.thecvf.com/content/CVPR2022/html/Ranasinghe_Self-Supervised_Video_Transformer_CVPR_2022_paper.html)| [SVT](https://github.com/kahnchana/svt)|CVPR'22-Oral|
Contrastive learning | Adversarial Defense | [Stylized Adversarial Training ](https://ieeexplore.ieee.org/document/9895320)| [SAT](https://github.com/Muzammal-Naseer/SAT)|IEEE-TPAMI'22|
Self-supervision| Adversarial Attack| [Adversarial Pixel Restoration as a Pretext Task for Transferable Perturbations](https://arxiv.org/abs/2207.08803)|[ARP](https://github.com/HashmatShadab/APR)|BMVC'22-Oral|
Self-supervision| Image Recognition | [How to Train Vision Transformer on Small-scale Datasets?](https://arxiv.org/abs/2210.07240v1)| [VSSD](https://github.com/hananshafi/vits-for-small-scale-datasets) |BMVC'22|
Self-distillation| Image Recognition (Domain Generalization) | [Self-Distilled Vision Transformer for Domain Generalization](https://openaccess.thecvf.com/content/ACCV2022/html/Sultana_Self-Distilled_Vision_Transformer_for_Domain_Generalization_ACCV_2022_paper.html)|[SDViT](https://github.com/maryam089/SDViT) | ACCV'22-Oral|
Attention Analysis | Understanding Vision Transformer| [Intriguing Properties of Vision Transformers](https://openreview.net/forum?id=o2mbl-Hmfgd)|[IPViT](https://github.com/Muzammal-Naseer/IPViT)| NeurIPS'21-Spotlight|
Self-ensemble | Adversarial Attack | [On Improving Adversarial Transferability of Vision Transformers](https://openreview.net/forum?id=D6nH3719vZy)|[ATViT](https://github.com/Muzammal-Naseer/ATViT)|ICLR'21-Spotlight|
Distribution matching | Adversarial Attack | [On Generating Transferable Targeted Perturbations](https://openaccess.thecvf.com/content/ICCV2021/html/Naseer_On_Generating_Transferable_Targeted_Perturbations_ICCV_2021_paper.html)| [TTP](https://github.com/Muzammal-Naseer/TTP) |ICCV'21|
Contrastive learning | Image Recognition | [Orthogonal Projection Loss ](https://openaccess.thecvf.com/content/ICCV2021/html/Ranasinghe_Orthogonal_Projection_Loss_ICCV_2021_paper.html)| [OPL](https://github.com/kahnchana/opl)|ICCV'21|
Self-supervision| Adversarial Defense | [A Self-supervised Approach for Adversarial Robustness](https://openaccess.thecvf.com/content_CVPR_2020/html/Naseer_A_Self-supervised_Approach_for_Adversarial_Robustness_CVPR_2020_paper.html)|[NRP](https://github.com/Muzammal-Naseer/NRP)|CVPR'20-Oral|
Relativistic optimization| Adversarial Attack | [Cross-Domain Transferability of Adversarial Perturbations](https://papers.nips.cc/paper/2019/hash/99cd3843754d20ec3c5885d805db8a32-Abstract.html) | [CDA](https://github.com/Muzammal-Naseer/Cross-Domain-Perturbations)|NeurIPS'19|
Gradient Smoothing| Adversarial Defense | [Local Gradients Smoothing: Defense Against Localized Adversarial Attacks](https://arxiv.org/abs/1807.01216) | [LGS](https://github.com/Muzammal-Naseer/LGS)| WACV'19|
